<!DOCTYPE html>
<html lang="en-US">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=1db909fa1f06ea7ae5dc1a7973fa98b17f58e65a" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>BERT | hahadsg’s note</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="BERT">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://hahadsg.github.io/MachineLearning/NLP/BERT.html">
<meta property="og:url" content="https://hahadsg.github.io/MachineLearning/NLP/BERT.html">
<meta property="og:site_name" content="hahadsg’s note">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-02-17T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="BERT">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-02-17T00:00:00+00:00","datePublished":"2020-02-17T00:00:00+00:00","headline":"BERT","mainEntityOfPage":{"@type":"WebPage","@id":"https://hahadsg.github.io/MachineLearning/NLP/BERT.html"},"url":"https://hahadsg.github.io/MachineLearning/NLP/BERT.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://hahadsg.github.io/">
          <h1>hahadsg's note</h1>
        </a>
        <h2></h2>
        
        
          <a href="https://github.com/hahadsg" class="button"><small>Follow me on</small> GitHub</a>
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1 id="bert">BERT</h1>

<h2 id="简介">简介</h2>

<p>NLP通常用预训练的语言表达来给下游使用，有两种常用的预训练的语言表达方式：feature-based和fine-tuning。feature-based方法，比如ELMo，将预训练好的词向量以及结构作为下游任务特征的一部分，下游任务的结构可以自定义，只要把ELMo的部分包含进去就行了（参数用预训练好的）。fine-tuning方法，比如GPT，将模型整体结构训练好，这个结构就不动了，下游任务按照它提供的方式组装，然后进行fine-tune。BERT属于fine-tuning类型的方法。</p>

<p>BERT的特点已经写在了它的名字中：Bidirectional Encoder Representations from Transformers。BERT是双向的，作为对比GPT是单向的，这是BERT的一个改进点，使用双向进行预训练效果会更好。使用了Transformer，比LSTM特征提取能力更强。另外BERT还使用了mask的方法进行训练，称为MLM（masked language model）。</p>

<p>下面将BERT分为pre-training和fine-tuning两个部分说明。pre-training表示预训练阶段，使用未标注的数据进行训练。fine-tuning表示微调阶段，使用预训练好的参数初始化模型，然后根据下游任务的标注数据进行微调。</p>

<h2 id="结构">结构</h2>

<p>BERT的结构跟GPT和ELMo结构的对比图如下：</p>

<p><img src="./assets/BERT/arch_compare.jpg" alt="drawing" width="700"></p>

<p>ELMo使用的两个单向LSTM的提取器，GPT使用了单向的Transformer。而BERT使用了双向的Transformer。Transformer的一些细节可以看<a href="/MachineLearning/NLP/transformer.html">这里</a>，其实BERT和GPT的Transformer就用了encoder部分。</p>

<p>假设\(L\)是Transformer的层数，\(H\)是隐藏层大小（也是embedding大小），\(A\)是multi-head的数量。BERT-BASIC结构的参数为：\(L=12, H=786, A=12\)；BERT-LARGE结构的参数为：\(L=24, H=1024, A=16\)。</p>

<h2 id="pre-training">Pre-training</h2>

<p>BERT用了MLM和NSP两种方法进行预训练。</p>

<h3 id="masked-lmmlm">Masked LM（MLM）</h3>

<p>通常的LM（language model）使用从左到右或者从右到左的方式提取特征，也就是逐次预测下一个词是什么，这是单向模型的模式（从Transformer的理论来看，就是要以这种方式训练）。</p>

<p>那么怎么才能进行双向的训练呢？MLM使用了mask的方式，就是把一整句话中15%的词替换成[MASK]的标记，然后预测[MASK]掉的地方，很像W2V中CBOW的方式。不过这里还有一个问题，在我们微调的时候，并没有[MASK]这个概念，而预训练时候会有很多[MASK]，所以预训练和微调的任务上就是有差别的。为了缓解这个问题（注意不是解决，而仅仅是缓解），BERT将mask掉的15%的词，其中80%用[MASK]标记代替，其中10%随机换成其他的词，10%不做改变（其实这里让我很费解，为什么会缓解，我暂时没找到让我满意的答案）。</p>

<h3 id="next-sencent-predictionnsp">Next Sencent Prediction（NSP）</h3>

<p>语言模型通常对于两个句子的关系没有理解能力（因为训练是没有提供这些数据），所以BERT就把这部分数据加入到了训练。NSP指的就是预测下一个句子，可以是Q&amp;A这种。具体的预测结构如下图：</p>

<p><img src="./assets/BERT/NSP.jpg" alt="drawing" width="600"></p>

<p>[CLS]表示分类问题的一个token，[SEP]表示句子间的分割token，最终模型的输出就是0或者1。可以看到，该任务中，还有位置embedding和句子的embedding。</p>

<h3 id="训练数据">训练数据</h3>

<p>论文提到一个细节，它使用wikipedia的时候只用了章节的文档，没有使用如标题、表格等文档，这让训练数据句子更加连贯。</p>

<h3 id="训练超参">训练超参</h3>

<p><strong>batch_size</strong>: 256 sequences (256 sequences * 512 tokens = 128000 tokens/batch)</p>

<p><strong>step_size</strong>: 1000000 steps (约40 epochs)</p>

<p><strong>optimizer(Adam)</strong>: 学习率1e-4线性衰减，warmup 10000 steps，\(\beta_1=0.9, \beta_2=0.999, L2=0.01\)</p>

<p>（注意L2这里论文是说L2 weight decay of 0.01，意思是用AdamW L2=0.01？还是指L2参数要衰减？）</p>

<p>激活函数用了GELU（而非RELU）。</p>

<h2 id="fine-tuning">Fine-tuning</h2>

<p>微调方法其实跟NSP的方法类似，input第一个token放上[CLS]，然后输出的第一个token就是一个分类问题的输出。</p>

<p>假设最后一层隐藏层输出的第一个向量为\(C \in \mathbb{R}^H\)（也就是对应着[CLS]的隐藏向量），分类问题的参数为\(W \in \mathbb{R}^{K\times H}\)，其中\(K\)是分类问题label数量，然后做一个分类问题的损失，进行训练即可。</p>

<h3 id="微调超参">微调超参</h3>

<p>大部分参数跟训练一样，除了下面几个参数值效果更好：</p>

<ul>
  <li>
<strong>batch_size:</strong> 16, 32</li>
  <li>
<strong>learning_rate:</strong> 5e-5, 3e-5, 2e-5</li>
  <li>
<strong>number of epochs:</strong> 2, 3, 4</li>
</ul>

<h3 id="任务适用范围">任务适用范围</h3>

<p>BERT可以用于分类问题，或者NER等，无法应用于机器翻译这种生成式的任务。</p>

<h1 id="参考">参考</h1>

<p><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

        </section>

        <aside id="sidebar">
          

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>

          <p>由于笔记大部分来自于我之前的gitbook，可能会因为markdown规范不同导致公式显示错误，如果有显示错误请<a href="https://github.com/hahadsg/hahadsg.github.io/issues">找我</a>，感谢</p>
        </aside>
      </div>
    </div>

    
  </body>
</html>
