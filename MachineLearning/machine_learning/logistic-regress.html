<!DOCTYPE html>
<html lang="en-US">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=1db909fa1f06ea7ae5dc1a7973fa98b17f58e65a" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>极大似然估计解释 | hahadsg’s note</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="极大似然估计解释">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://hahadsg.github.io/MachineLearning/machine_learning/logistic-regress.html">
<meta property="og:url" content="https://hahadsg.github.io/MachineLearning/machine_learning/logistic-regress.html">
<meta property="og:site_name" content="hahadsg’s note">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="极大似然估计解释">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"极大似然估计解释","url":"https://hahadsg.github.io/MachineLearning/machine_learning/logistic-regress.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://hahadsg.github.io/">
          <h1>hahadsg's note</h1>
        </a>
        <h2></h2>
        
        
          <a href="https://github.com/hahadsg" class="button"><small>Follow me on</small> GitHub</a>
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h2 id="极大似然估计解释">极大似然估计解释</h2>

<p>\(\hat{y}\)是给定\(x\)对\(y=1\)的估计:
\(\hat{y} = P(y=1\rvert x)\)</p>

<p>所以，\(1-\hat{y} = P(y=0\rvert x)\)</p>

<p>归纳起来，\(P(y\rvert x)=\hat{y}^y (1-\hat{y})^{1-y}\)</p>

<p>则\(log(P(y\rvert x))=y*log(\hat{y})+(1-y)*log(1-\hat{y})\)</p>

<p>那对于所有的样本，用极大似然估计：</p>

\[L=\prod P(y^{(i)}\rvert x^{(i)})\]

<p>取对数似然，</p>

\[l=log(\prod P(y^{(i)}\rvert x^{(i)}))=\sum y*log(\hat{y})+(1-y)*log(1-\hat{y})\]

<p>由于要最大化\(l\)，那我们的损失函数要加上负号，</p>

\[J=-\sum y*log(\hat{y})+(1-y)*log(1-\hat{y})\]

<h2 id="求导">求导</h2>

\[z = Wx + b\]

\[\hat{y} = \sigma(z) = \sigma(Wx + b)\]

\[J = -y \bullet log(\hat{y}) - (1-y) \bullet log(1-\hat{y})\]

\[\frac{\partial J}{\partial \hat{y}} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})}\]

\[\frac{\partial \hat{y}}{\partial z} = \hat{y}(1-\hat{y})\]

\[\frac{\partial J}{\partial z} = \frac{\partial J}{\partial \hat{y}} \bullet \frac{\partial \hat{y}}{\partial z} = \hat{y} - y\]

\[\frac{\partial J}{\partial W} = \frac{\partial J}{\partial z} \bullet \frac{\partial z}{\partial W} = (\hat{y} - y)x\]

\[\frac{\partial J}{\partial b} = \frac{\partial J}{\partial z} \bullet \frac{\partial z}{\partial b} = \hat{y} - y\]

<h2 id="softmax">Softmax</h2>

\[\begin{aligned}
&amp;\hat{y_j} = \frac{e^{z_j}}{\sum\limits^K_k e^{z_k}} \\
&amp;L = -\sum\limits^M_i \sum\limits^K_k 1\{y^{(i)}=k\} ln\hat{y}^{(i)} \\
\end{aligned}\]

<p>对于单个样本，第j类求导：</p>

\[\begin{aligned}
&amp;\frac{ \partial{L_i} }{ \partial{\hat{y_j}} } = \frac{1}{ \hat{y_j} } \\
&amp;\frac{ \partial{\hat{y_j}} }{ \partial{z_j} } = \frac{ \partial }{ \partial{z_j} } \frac{ e^{z_j} }{ \sum\limits^K_k e^{z_k} } \\
&amp;\qquad = \frac{ e^{z_j} . \sum\limits^K_k e^{z_k} - e^{z_j} . e^{z_j} }{ {(\sum\limits^K_k e^{z_k})}^2 } \\
&amp;\qquad = \hat{y_j} . (1 - \hat{y_j}) \\
&amp;\frac{ \partial{L_i} }{ \partial{z_j} } = 1 - \hat{y_j} \\
\end{aligned}\]

<h2 id="softmax-and-sigmoid">Softmax and Sigmoid</h2>

<p>Sigmoid: \(\hat{y} = \frac{1}{1+e^{-z}}\)</p>

<p>Softmax: \(\hat{y_j} = \frac{e^{z_j}}{\sum e^{z_k}}\)</p>

<p>对于Softmax类别数量为2的情况，</p>

\[\hat{y_1} = \frac{e^{z_1}}{e^{z_0} + e^{z_1}} = \frac{1}{e^{z_0-z_1} + 1}\]

<p>所以，Sigmoid是Softmax类别数量为2的特殊情况，但是Softmax要比Sigmoid浪费2倍的参数空间</p>

<h2 id="logstic-loss的形式">logstic loss的形式</h2>

<p>label是0和1时，\(L = -ylog\hat{y} -(1-y)log(1-\hat{y})\)</p>

<p>label是-1和1时，\(\hat{y} = \sigma(y_r), L = -log(\sigma(yy_r))\)，或者\(L = log(1+e^{-yy_r})\)</p>

<p>作为对比，hinge loss（label是-1和1）是\(L = max(1-y\hat{y}, 0)\)</p>

        </section>

        <aside id="sidebar">
          

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>

          <p>由于笔记大部分来自于我之前的gitbook，可能会因为markdown规范不同导致公式显示错误，如果有显示错误请<a href="https://github.com/hahadsg/hahadsg.github.io/issues">找我</a>，感谢</p>
        </aside>
      </div>
    </div>

    
  </body>
</html>
