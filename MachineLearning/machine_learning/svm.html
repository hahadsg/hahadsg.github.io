<!DOCTYPE html>
<html lang="en-US">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=1db909fa1f06ea7ae5dc1a7973fa98b17f58e65a" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>拉格朗日对偶性 | hahadsg’s note</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="拉格朗日对偶性">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://hahadsg.github.io/MachineLearning/machine_learning/svm.html">
<meta property="og:url" content="https://hahadsg.github.io/MachineLearning/machine_learning/svm.html">
<meta property="og:site_name" content="hahadsg’s note">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="拉格朗日对偶性">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"拉格朗日对偶性","url":"https://hahadsg.github.io/MachineLearning/machine_learning/svm.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://hahadsg.github.io/">
          <h1>hahadsg's note</h1>
        </a>
        <h2></h2>
        
        
          <a href="https://github.com/hahadsg" class="button"><small>Follow me on</small> GitHub</a>
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h2 id="拉格朗日对偶性">拉格朗日对偶性</h2>

<ul>
  <li><strong>原始问题</strong></li>
</ul>

\[\begin{aligned}
&amp;\min \quad f(x) \\
&amp;s.t. \quad c_i(x) \le 0, \quad i=1,2,...,k \\
&amp;\qquad \quad h_j(x) = 0, \quad j=1,2,...,l  \\
\end{aligned}\]

<p>引进拉格朗日函数，
\(L(x, \alpha, \beta)=f(x) + \sum\limits^k_{i=1}\alpha_i c_i(x) + \sum\limits^l_{j=1}\beta_j h_j(x)\)
这里，\(\alpha_i \ge 0\)</p>

<p>考虑函数，
\(\theta_P(x) = \max\limits_{\alpha , \beta} L(x, \alpha, \beta)\)
这里\(P\)代表原始问题</p>

<p>求解原问题，即求解
\(\min\limits_{x} \theta_P(x) = \min\limits_{x} \max\limits_{\alpha , \beta} L(x, \alpha, \beta)\)</p>

<p>定义原始问题的最优解
\(p^* = \min\limits_{x} \theta_P(x)\)</p>

<ul>
  <li><strong>对偶问题</strong></li>
</ul>

<p>\(\theta_D(\alpha, \beta) = \min\limits_{\alpha, \beta}L(x, \alpha, \beta)\)
这里\(D\)代表对偶问题</p>

<p>对偶问题的最优解，
\(d^* = \theta_D(\alpha, \beta)\)</p>

<ul>
  <li><strong>原始问题和对偶问题的关系</strong></li>
</ul>

\[d^* \le p^*\]

<ul>
  <li><strong>KKT条件</strong></li>
</ul>

<p>满足KKT条件时，\(d^* = p^*\)</p>

<h2 id="线性可分支持向量机硬间隔最大化">线性可分支持向量机（硬间隔最大化）</h2>

<p>下图中的三个点，A距离分类边界很远，我们可以很确定A是X类；而C距离分类边界很近了，它属于X类的确信度就不是很高了；而B在AC之间，确信度也在AC之间。所以我们想让离分类边界最近的点，尽可能离分类边界远</p>

<p><img src="/MachineLearning/machine_learning/assets/svm/abc.png" alt=""></p>

<p>我们定义函数间隔：
\(\hat\gamma_i = |wx_i+b| = y_i(wx_i+b)\)</p>

<p>几何间隔：
\(\gamma_i = \frac{|wx_i+b|}{\lVert w \rVert} = \frac{y_i(wx_i+b)}{\lVert w \rVert}\)</p>

<p>它们之间的关系是：
\(\gamma_i = \frac{\hat\gamma_i}{\lVert w \rVert}\)</p>

<p><img src="/MachineLearning/machine_learning/assets/svm/geometric_margin.png" alt=""></p>

<p>我们先假设数据线性可分，基于上面的想法，我们想让离分类边界（几何间隔）最近的点，尽可能离分类边界远，也就是使，</p>

\[\begin{aligned}
&amp;\max \quad \gamma \\
&amp;s.t. \quad \frac{y_i(wx_i+b)}{\lVert w \rVert} \ge \gamma 
\end{aligned}\]

<p>转化成函数间隔后，</p>

\[\begin{aligned}
&amp;\max \quad \frac{\hat\gamma}{\lVert w \rVert} \\
&amp;s.t. \quad y_i(wx_i+b) \ge \hat\gamma 
\end{aligned}\]

<p>由于我们求\(\frac{\hat\gamma}{\lVert w \rVert}\)最大时，\(\hat\gamma\)的取值不会影响这个最优化问题，所以我们取\(\hat\gamma=1\)，则最优化问题变成了，</p>

\[\begin{aligned}
&amp;\min \quad \frac{1}{2}\lVert w \rVert^2 \\
&amp;s.t. \quad y_i(wx_i+b) \ge 1 
\end{aligned}\]

<p>根据这个最优化问题，我们列出拉格朗日函数，</p>

\[L(w,b,\alpha) = \frac{1}{2}\lVert w \rVert^2 + \sum\limits^N_{i=1}\alpha_i [y_i(wx_i+b)-1]\]

<p>那么，原问题就是，</p>

\[p^* = \min\limits_{w,b} \max\limits_{\alpha} L(w,b,\alpha)\]

<p>它的对偶问题是，</p>

\[d^* = \max\limits_{\alpha} \min\limits_{w,b} L(w,b,\alpha)\]

<p>由于优化问题满足KKT条件，所以可以直接求解对偶问题
那么我们先求\(\min\limits_{w,b} L(w,b,\alpha)\)</p>

\[\frac{\partial L(w,b,\alpha)}{\partial w} = w - \sum\limits^N_{i=1}\alpha_i y_i x_i = 0 \\
\frac{\partial L(w,b,\alpha)}{\partial b} = - \sum\limits^N_{i=1}\alpha_i y_i = 0 \\\]

<p>得到，</p>

\[w = \sum\limits^N_{i=1}\alpha_i y_i x_i \\
\sum\limits^N_{i=1}\alpha_i y_i = 0 \\\]

<p>代入\(L(w,b,\alpha)\)中，得到</p>

\[L(w,b,\alpha) = \sum\limits^N_{i=1}\alpha_i - \frac{1}{2} \sum\limits^N_{i=1} \sum\limits^N_{j=1} \alpha_i \alpha_j y_i y_j x_i x_j\]

<p>然后再进行极大化过程，</p>

\[\begin {aligned}
&amp;\max \quad \sum\limits^N_{i=1}\alpha_i - \frac{1}{2} \sum\limits^N_{i=1} \sum\limits^N_{j=1} \alpha_i \alpha_j y_i y_j x_i x_j \\
&amp;s.t. \quad \sum\limits^N_{i=1}\alpha_i y_i = 0 \\
&amp;\qquad \quad \alpha_i \ge 0, i=1,2,...,N 
\end{aligned}\]

<p>根据上式可以求得\(a^*\)，则可以进一步求得\(w^*,b^*\)</p>

<p>最终决策函数为，</p>

\[f(x) = sign(\sum\limits^N_{i=1} \alpha_i^* y_i (x\bullet x_i)+b^*)\]

<h2 id="线性支持向量机软间隔最大化">线性支持向量机（软间隔最大化）</h2>

<p>对于实践中的数据，很多时候都是线性不可分的，所以这时候我们引入一个松弛因子\(\xi\)，则点的约束条件变成了，</p>

\[\hat\gamma_i = y_i(wx_i+b) \ge 1-\xi_i\]

<p>这时候优化问题变成了，</p>

\[\begin {aligned}
&amp;\min \quad \frac{1}{2}\lVert w \rVert^2 + C\sum\limits^N_{i=1}\xi_i \\
&amp;s.t. \quad y_i(wx_i+b) \ge 1 
\end{aligned}\]

<p>求解后得到，</p>

\[\begin {aligned}
&amp;\max \quad \sum\limits^N_{i=1}\alpha_i - \frac{1}{2} \sum\limits^N_{i=1} \sum\limits^N_{j=1} \alpha_i \alpha_j y_i y_j x_i x_j \\
&amp;s.t. \quad \sum\limits^N_{i=1}\alpha_i y_i = 0 \\
&amp;\qquad \quad 0 \le \alpha_i \le C, i=1,2,...,N 
\end{aligned}\]

<p>可以看到，\(\xi\)不见了，它比线性可分支持向量机多了一个\(\alpha_i \le C\)
不过需要注意的是\(b^*\)的公式也变了</p>

<h2 id="非线性支持向量机核技巧">非线性支持向量机（核技巧）</h2>

<p>可以看到下图中的数据，可以用一个椭圆形的分类边界将数据分类，
也就是将原来的\(x\)映射到二维\(z=x^2\)可以将数据很好的分割</p>

<p><img src="/MachineLearning/machine_learning/assets/svm/kernel_trick.jpg" alt=""></p>

<p>根据上面的思路，我们将\(x\)映射到更高维，\(\phi(x)\)，
则\(wx+b\)就变成了\(w\phi(x)+b\)</p>

<p><strong>核函数</strong></p>

<p>我们定义核函数\(K(x,z)=\phi(x)\bullet\phi(z)\)</p>

<p>我们看个例子，假设核函数\(K(x,z)=(x\bullet z)^2\)，\(x,z\)均是2维的，
那么\(\phi(x)=((x^{(1)})^2, \sqrt{2}x^{(1)}x^{(2)}, (x^{(2)})^2)\)，可以使得核函数成立</p>

<p>那这个时候，我们就可以不用求\(\phi\)具体是什么，只要直接使用\(K(x,z)\)</p>

<p>当然核函数需要满足一定条件才能这么干，常用的核函数有，
多项式核函数：\(K(x,z)=(x\bullet z+1)^p\)
高斯径向基函数：\(K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2})\)</p>

<p><strong>在SVM中使用</strong></p>

<p>决策函数为，</p>

\[f(x) = sign(\sum\limits^N_{i=1} \alpha_i^* y_i (x\bullet x_i)+b^*)\]

<p>我们映射到高维，</p>

\[f(x) = sign(\sum\limits^N_{i=1} \alpha_i^* y_i (\phi(x)\bullet \phi(x_i))+b^*)\]

<p>转化成核函数，</p>

\[f(x) = sign(\sum\limits^N_{i=1} \alpha_i^* y_i K(x, x_i)+b^*)\]

<h2 id="svm对比logistic-regression">SVM对比Logistic Regression</h2>

<p>1、SVM只关心一部分点，而LR关心所有的点</p>

<p>SVM增加点后的状况
<img src="/MachineLearning/machine_learning/assets/svm/svm_change.png" alt=""></p>

<p>LR增加点后的状况
<img src="/MachineLearning/machine_learning/assets/svm/lr_change.png" alt=""></p>


        </section>

        <aside id="sidebar">
          

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>

          <p>由于笔记大部分来自于我之前的gitbook，可能会因为markdown规范不同导致公式显示错误，如果有显示错误请<a href="https://github.com/hahadsg/hahadsg.github.io/issues">找我</a>，感谢</p>
        </aside>
      </div>
    </div>

    
  </body>
</html>
